\documentclass[10pt,letterpaper,onecolumn,draftclsnofoot]{IEEEtran}
\usepackage[margin=0.75in]{geometry}
\usepackage{listings}
\usepackage{color}
\usepackage{longtable}
\usepackage{tabu}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  columns=flexible,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=4
}

\begin{document}
\begin{titlepage}
  \title{CS 434 - Spring 2016 - Final Project Report}
  \author{Garrett Smith, Cody Malick\\
  \texttt{smithgar@oregonstate.edu} | \texttt{malickc@oregonstate.edu}}
  \maketitle
  \vspace*{4cm}
  \begin{abstract}
      \noindent In this report, we examine four different algorithms for machine
      learning: linear regression, logistic regression, support vector machine,
      and decision tree. We apply these four algorithms to the same problem, spam
      classification, and examine how they perform, how they perform in different
      conditions, and how they compare to each other.

  \end{abstract}
\end{titlepage}

\tableofcontents
\clearpage
\section{Introduction}
Classification is a common problem in machine learning. There are many approaches,
some simple, some more complex, that all involve sorting a given set of items into
two or more classifications. The goal, of course, being able to quickly and efficiently
classify large sets of items into their correct classes. In our final project, we
have examined four seperate classification algorithms, and applied them all to the
same problem of classifying spam.

\section{The Data}
Our data set is a pre-processed set of spam from University of California Irvine,
the \texttt{Spambase Data Set}.\cite{spambase} The data set is comprised of the
following:
\begin{description}
  \item Number of Instances: 4601
  \item Number of Attributes: 57
  \begin{itemize}
    \item 48 continous real [0,100] attributes of type word\_freq\_WORD
    percentage of words in the e-mail that match WORD, i.e. 100 * (number of
    times the WORD appears in the e-mail) / total number of words in e-mail. A
    ``word'' in this case is any string of alphanumeric characters bounded by
    non-alphanumeric characters or end-of-string.

    \item 6 continuous real [0,100] attributes of type char\_freq\_CHAR]
    = percentage of characters in the e-mail that match CHAR, i.e. 100 *
    (number of CHAR occurences) / total characters in e-mail

    \item 1 continuous real [1,...] attribute of type capital\_run\_length\_average
    = average length of uninterrupted sequences of capital letters

    \item 1 continuous integer [1,...] attribute of type capital\_run\_length\_longest
    = length of longest uninterrupted sequence of capital letters

    \item 1 continuous integer [1,...] attribute of type capital\_run\_length\_total
    = sum of length of uninterrupted sequences of capital letters
    = total number of capital letters in the e-mail

    \item 1 nominal {0,1} class attribute of type spam
    = denotes whether the e-mail was considered spam (1) or not (0), i.e.
    unsolicited commercial e-mail.
  \end{itemize}
  \item Class Distribution
  \begin{itemize}
      \item Spam: 1813 (39.4\%)
      \item Non-Spam: 2788 (60.6\%)
  \end{itemize}
\end{description}

This data was great for a comparison set because we could look at the exact distribution
of data, what kind of data it was, what the different attributes meant, etc. Now
that we have the layout of the data, next we will examine our first algorithm,
linear regression.

\section{Linear Regression}
Linear regression is a very simple algorithm, designed to handle data in very simple
dimensions. We ran into a lot of problems trying to implement this algorithm over
a data set with over fifty features. After some trial and error, we decided that
applying this algorithm to a feature set this complex didn't make a lot of sense.
We were hitting a few percetage points in accuracy, and we believe that it was
purely luck that we got that many. Instead of thoroughly investigating linear regression,
we added support vector machine to our lineup of algorithms to examine. Before
jumping into a support vector machine, we will now examine the next closest thing
to linear regression, logistic regression.

\section{Logistic Regression}
\section{Support Vector Machine}
\section{Decision Tree}

\section{Conclusion}

\clearpage
\section{Appendix A - Linux Structs}
All following structs have been pulled from the classroom text. \cite{robertlove2010}
\begin{lstlisting}

for future code samples

\end{lstlisting}

\section{Bibliography}
\bibliographystyle{IEEEtran}
\bibliography{final}

\end{document}
